{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2799ec-7453-434e-8605-7a5bfe39b578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\leoci\\onedrive\\área de trabalho\\scapegoat\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\leoci\\onedrive\\área de trabalho\\scapegoat\\.venv\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\leoci\\onedrive\\área de trabalho\\scapegoat\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\leoci\\onedrive\\área de trabalho\\scapegoat\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\leoci\\onedrive\\área de trabalho\\scapegoat\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91c3beb-ec28-42e7-9fc7-93a1d4cf06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUPYTER-FRIENDLY DATA PREP FOR STANCE CLASSIFICATION\n",
    "# - Merge multiple JSON files (each a list of dicts with 'stance')\n",
    "# - Add `case` from filename\n",
    "# - Dedup by `id`, shuffle, save merged JSON/JSONL\n",
    "# - Stratified split (train/val/test) by `stance`\n",
    "# - Save label_map, class_counts, class_weights, per_case_counts, summary\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json, random\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def _load_json_list(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(f\"{path} is not a JSON list\")\n",
    "    return data\n",
    "\n",
    "def _normalize_record(rec: Dict[str, Any], case_name: str) -> Dict[str, Any]:\n",
    "    out = dict(rec)\n",
    "    out[\"case\"] = case_name\n",
    "    if isinstance(out.get(\"stance\"), str):\n",
    "        out[\"stance\"] = out[\"stance\"].strip().lower()\n",
    "    return out\n",
    "\n",
    "def _write_json(path: Path, obj: Any):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def _write_jsonl(path: Path, rows: List[Dict[str, Any]]):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _compute_class_weights(labels: List[str]) -> Dict[str, float]:\n",
    "    counts = Counter(labels)\n",
    "    total = sum(counts.values())\n",
    "    k = len(counts)\n",
    "    return {lbl: (total / (k * cnt)) for lbl, cnt in counts.items() if cnt > 0}\n",
    "\n",
    "def prepare_merge_and_split(\n",
    "    inputs: List[str],\n",
    "    output_dir: str,\n",
    "    seed: int = 42,\n",
    "    val_size: float = 0.10,\n",
    "    test_size: float = 0.10,\n",
    "    shuffle: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs: list of JSON file paths (each a list of dicts with 'stance')\n",
    "        output_dir: where to save outputs\n",
    "        seed, val_size, test_size: split controls\n",
    "        shuffle: shuffle before splitting\n",
    "\n",
    "    Returns:\n",
    "        dict with paths, counts, weights, and split sizes\n",
    "    \"\"\"\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Load + tag case from filename\n",
    "    merged = []\n",
    "    for p in inputs:\n",
    "        pth = Path(p)\n",
    "        case_name = pth.stem.replace(\"tweets_\", \"\").replace(\"_final_for_bert\", \"\")\n",
    "        data = _load_json_list(pth)\n",
    "        merged.extend([_normalize_record(r, case_name) for r in data])\n",
    "\n",
    "    # Dedup by 'id'\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for r in merged:\n",
    "        tid = r.get(\"id\")\n",
    "        if tid in seen:\n",
    "            continue\n",
    "        seen.add(tid)\n",
    "        deduped.append(r)\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        random.shuffle(deduped)\n",
    "\n",
    "    # Ensure keys\n",
    "    required = [\"id\", \"text\", \"clean_text\", \"stance\", \"case\"]\n",
    "    for r in deduped:\n",
    "        for k in required:\n",
    "            r.setdefault(k, None)\n",
    "\n",
    "    # Save merged\n",
    "    merged_json = out_dir / \"merged_scapegoat_stance.json\"\n",
    "    merged_jsonl = out_dir / \"merged_scapegoat_stance.jsonl\"\n",
    "    _write_json(merged_json, deduped)\n",
    "    _write_jsonl(merged_jsonl, deduped)\n",
    "\n",
    "    # Labels, maps, counts\n",
    "    labels = [r[\"stance\"] for r in deduped if r.get(\"stance\") is not None]\n",
    "    if not labels:\n",
    "        raise ValueError(\"No 'stance' labels found.\")\n",
    "\n",
    "    label_order = sorted(set(labels))  # deterministic\n",
    "    label_map = {lbl: i for i, lbl in enumerate(label_order)}\n",
    "    _write_json(out_dir / \"label_map.json\", label_map)\n",
    "\n",
    "    class_counts = Counter(labels)\n",
    "    _write_json(out_dir / \"class_counts.json\", class_counts)\n",
    "\n",
    "    class_weights = _compute_class_weights(labels)\n",
    "    _write_json(out_dir / \"class_weights.json\", class_weights)\n",
    "\n",
    "    per_case = defaultdict(lambda: Counter())\n",
    "    for r in deduped:\n",
    "        per_case[r[\"case\"]][r.get(\"stance\")] += 1\n",
    "    per_case_out = {case: dict(cnt) for case, cnt in per_case.items()}\n",
    "    _write_json(out_dir / \"per_case_counts.json\", per_case_out)\n",
    "\n",
    "    # Stratified splits\n",
    "    holdout = val_size + test_size\n",
    "    if not (0 < holdout < 1):\n",
    "        raise ValueError(\"val_size + test_size must be in (0,1).\")\n",
    "\n",
    "    rows = [r for r in deduped if r.get(\"stance\") in label_map]\n",
    "    y = [r[\"stance\"] for r in rows]\n",
    "    X = list(range(len(rows)))\n",
    "\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "        X, y, test_size=holdout, random_state=seed, stratify=y\n",
    "    )\n",
    "    test_ratio = 0 if test_size == 0 else test_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_tmp, y_tmp, test_size=test_ratio, random_state=seed, stratify=y_tmp if test_ratio > 0 else None\n",
    "    )\n",
    "\n",
    "    train_rows = [rows[i] for i in X_train]\n",
    "    val_rows   = [rows[i] for i in X_val]\n",
    "    test_rows  = [rows[i] for i in X_test]\n",
    "\n",
    "    _write_jsonl(out_dir / \"merged_train.jsonl\", train_rows)\n",
    "    _write_jsonl(out_dir / \"merged_val.jsonl\",   val_rows)\n",
    "    _write_jsonl(out_dir / \"merged_test.jsonl\",  test_rows)\n",
    "\n",
    "    # Summary (also save)\n",
    "    summary = {\n",
    "        \"total_after_dedup\": len(deduped),\n",
    "        \"label_order\": label_order,\n",
    "        \"class_counts\": dict(class_counts),\n",
    "        \"class_weights\": {k: round(v, 6) for k, v in class_weights.items()},\n",
    "        \"per_case_counts\": per_case_out,\n",
    "        \"splits\": {\"train\": len(train_rows), \"val\": len(val_rows), \"test\": len(test_rows)},\n",
    "        \"files\": {\n",
    "            \"merged_json\": str(merged_json),\n",
    "            \"merged_jsonl\": str(merged_jsonl),\n",
    "            \"label_map\": str(out_dir / \"label_map.json\"),\n",
    "            \"class_counts\": str(out_dir / \"class_counts.json\"),\n",
    "            \"class_weights\": str(out_dir / \"class_weights.json\"),\n",
    "            \"per_case_counts\": str(out_dir / \"per_case_counts.json\"),\n",
    "            \"train\": str(out_dir / \"merged_train.jsonl\"),\n",
    "            \"val\": str(out_dir / \"merged_val.jsonl\"),\n",
    "            \"test\": str(out_dir / \"merged_test.jsonl\"),\n",
    "        },\n",
    "    }\n",
    "    with (out_dir / \"summary.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(summary, ensure_ascii=False, indent=2) + \"\\n\")\n",
    "\n",
    "    # Nice print\n",
    "    print(f\"Total after dedup: {summary['total_after_dedup']}\")\n",
    "    for lbl in summary[\"label_order\"]:\n",
    "        print(f\"  {lbl}: {summary['class_counts'].get(lbl,0)}  (weight={summary['class_weights'][lbl]:.4f})\")\n",
    "    print(f\"Splits -> Train: {summary['splits']['train']}  Val: {summary['splits']['val']}  Test: {summary['splits']['test']}\")\n",
    "    print(f\"Wrote outputs to: {out_dir.resolve()}\")\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00300098-d8a0-47f8-a5a3-e8828c585da3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\data\\\\processed\\\\tweets_monark_final_for_bert.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m----------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m summary = \u001b[43mprepare_merge_and_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/processed/tweets_monark_final_for_bert.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/processed/tweets_wagner_schwartz_final_for_bert.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m summary  \u001b[38;5;66;03m# shows paths, counts, weights, split sizes\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mprepare_merge_and_split\u001b[39m\u001b[34m(inputs, output_dir, seed, val_size, test_size, shuffle)\u001b[39m\n\u001b[32m     69\u001b[39m     pth = Path(p)\n\u001b[32m     70\u001b[39m     case_name = pth.stem.replace(\u001b[33m\"\u001b[39m\u001b[33mtweets_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m_final_for_bert\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     data = \u001b[43m_load_json_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     merged.extend([_normalize_record(r, case_name) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data])\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Dedup by 'id'\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36m_load_json_list\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_json_list\u001b[39m(path: Path):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     16\u001b[39m         data = json.load(f)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\pathlib.py:1044\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1043\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '..\\\\data\\\\processed\\\\tweets_monark_final_for_bert.json'"
     ]
    }
   ],
   "source": [
    "summary = prepare_merge_and_split(\n",
    "    inputs=[\n",
    "        \"../data/processed/tweets_monark_final_for_bert.json\",\n",
    "        \"../data/processed/tweets_wagner_schwartz_final_for_bert.json\",\n",
    "    ],\n",
    "    output_dir=\"../data/train\",\n",
    "    seed=42,\n",
    "    val_size=0.10,\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "summary  # shows paths, counts, weights, split sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080df02-9984-4cf6-b665-c66c607c5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.exists(\"../data/processed/tweets_monark_final_for_bert.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb4f9c-5fca-42fc-a864-d85ae243bb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
