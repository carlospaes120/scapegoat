{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24c60c4d-2674-47be-bb7e-2ab1a2c6cb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects: 1001\n"
     ]
    }
   ],
   "source": [
    "#convert jsonl files on json\n",
    "import json\n",
    "\n",
    "input_path = '../data/interim/tweets_cleaned_wagner_schwartz_with_stance.jsonl'\n",
    "output_path = '../data/interim/tweets_cleaned_wagner_schwartz_classified.json'\n",
    "n_loop = 1000\n",
    "tasks = []\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    count = 0\n",
    "    for line in infile:\n",
    "        if count <= n_loop:\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            if obj[\"stance\"] != \"\":\n",
    "                tasks.append({\n",
    "                    \"text\": obj[\"text\"],\n",
    "                    \"cleaned_text\": obj[\"cleaned_text\"],\n",
    "                    \"id\": obj[\"id\"],\n",
    "                    \"stance\": obj[\"stance\"]\n",
    "                })\n",
    "                count+= 1\n",
    "    print(f\"Number of objects: {count}\")\n",
    "    \n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(tasks, outfile, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370fa245-230b-4755-af4b-781c4324d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates. Kept 4795 unique items.\n"
     ]
    }
   ],
   "source": [
    "#remove duplicated tweets of json files\n",
    "import json\n",
    "\n",
    "def remove_duplicates_keep_classified(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ JSON is malformed: {e}\")\n",
    "            return\n",
    "\n",
    "    seen = {}\n",
    "\n",
    "    for idx, item in enumerate(data, 1):\n",
    "        if not isinstance(item, dict) or \"id\" not in item:\n",
    "            print(f\"âš ï¸ Skipping malformed object at index {idx}\")\n",
    "            continue\n",
    "\n",
    "        tid = item[\"id\"]\n",
    "\n",
    "        if tid not in seen:\n",
    "            seen[tid] = item\n",
    "        else:\n",
    "            # prefer classified version (stance != \"\")\n",
    "            if seen[tid].get(\"stance\", \"\") == \"\" and item.get(\"stance\", \"\") != \"\":\n",
    "                seen[tid] = item\n",
    "            # if both classified, keep the first one\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(seen.values()), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Removed duplicates. Kept {len(seen)} unique items.\")\n",
    "\n",
    "# Example usage\n",
    "remove_duplicates_keep_classified(\n",
    "    \"tweets_for_labelstudio_monark.json\",\n",
    "    \"tweets_for_labelstudio_monark_unique.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ba36e2-e81f-4e73-bc75-f8bc08bb7b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 4794\n",
      "Classified tweets: 600\n",
      "Unclassified tweets: 4194\n"
     ]
    }
   ],
   "source": [
    "#count classified tweets\n",
    "import json\n",
    "\n",
    "def count_classified(input_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    total = len(data)\n",
    "    classified = sum(1 for item in data if item.get(\"stance\", \"\").strip() != \"\")\n",
    "    unclassified = total - classified\n",
    "\n",
    "    print(f\"Total tweets: {total}\")\n",
    "    print(f\"Classified tweets: {classified}\")\n",
    "    print(f\"Unclassified tweets: {unclassified}\")\n",
    "\n",
    "# Example usage\n",
    "count_classified(\"tweets_for_labelstudio_monark_unique.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5297f30c-3f6f-4847-9d99-db0c5719b019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 600 classified tweets into 'tweets_for_labelstudio_monark_classified.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def select_classified(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # keep only tweets with stance filled\n",
    "    classified = [item for item in data if item.get(\"stance\", \"\").strip() != \"\"]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(classified, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Extracted {len(classified)} classified tweets into '{output_file}'.\")\n",
    "\n",
    "# Example usage\n",
    "select_classified(\n",
    "    \"tweets_for_labelstudio_monark_unique.json\",\n",
    "    \"tweets_for_labelstudio_monark_classified.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6cf5ae0-84f3-400c-a908-d523afb03721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed 1001 items â†’ ../data/interim/tweets_wagner_schwartz_final_for_bert.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "URL_RE = re.compile(r'https?://\\S+', flags=re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r'@([A-Za-z0-9_]{1,15})')  # Twitter/X handle up to 15 chars\n",
    "HASHTAG_RE = re.compile(r'#([\\w]+)', flags=re.UNICODE)  # \\w covers Unicode letters/digits/_\n",
    "\n",
    "def unique_preserve_order(items: List[str]) -> List[str]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for it in items:\n",
    "        if it.lower() not in seen:\n",
    "            seen.add(it.lower())\n",
    "            out.append(it)\n",
    "    return out\n",
    "\n",
    "def strip_extra_spaces(s: str) -> str:\n",
    "    # collapse whitespace and trim\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "def build_fields_from_text(raw_text: str):\n",
    "    # 1) Extract mentions / hashtags (lowercased, de-duplicated, order-preserving)\n",
    "    mentions = unique_preserve_order([m.lower() for m in MENTION_RE.findall(raw_text or \"\")])\n",
    "    hashtags = unique_preserve_order([h.lower() for h in HASHTAG_RE.findall(raw_text or \"\")])\n",
    "\n",
    "    # 2) Remove URLs\n",
    "    no_urls = URL_RE.sub(' ', raw_text or '')\n",
    "\n",
    "    # 3) Build TEXT FOR MODEL:\n",
    "    #    remove @mentions but KEEP hashtags inline\n",
    "    text_model = re.sub(r'@([A-Za-z0-9_]{1,15})', ' ', no_urls)\n",
    "    text_model = strip_extra_spaces(text_model)\n",
    "\n",
    "    # 4) Build a CLEAN TEXT (optional): remove mentions AND hashtags\n",
    "    clean_text = HASHTAG_RE.sub(' ', re.sub(r'@([A-Za-z0-9_]{1,15})', ' ', no_urls))\n",
    "    clean_text = strip_extra_spaces(clean_text)\n",
    "\n",
    "    return text_model, clean_text, hashtags, mentions\n",
    "\n",
    "def transform_dataset(input_file: str, output_file: str):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    out = []\n",
    "    for item in data:\n",
    "        # prefer original \"text\" for parsing hashtags/mentions\n",
    "        raw_text = item.get(\"text\", \"\") or \"\"\n",
    "        stance = item.get(\"stance\", \"\")\n",
    "\n",
    "        text_model, clean_text, hashtags, mentions = build_fields_from_text(raw_text)\n",
    "\n",
    "        out.append({\n",
    "            \"id\": item.get(\"id\"),\n",
    "            # text for the model: URLs + @mentions removed, hashtags KEPT inline\n",
    "            \"text\": text_model,\n",
    "            # optional extra clean version (no urls/@/hashtags) if you want to experiment\n",
    "            \"clean_text\": clean_text,\n",
    "            \"hashtags\": hashtags,    # e.g. [\"monark\", \"flowpodcast\"]\n",
    "            \"mentions\": mentions,    # e.g. [\"ratoborrachudo\", \"igor_3k\"]\n",
    "            \"stance\": stance\n",
    "        })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Transformed {len(out)} items â†’ {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "transform_dataset(\"../data/interim/tweets_cleaned_wagner_schwartz_classified.json\", \"../data/interim/tweets_wagner_schwartz_final_for_bert.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5bd57d2-0524-4ea0-a35b-eb37a9b25b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Processing: search_response_%22La+B%C3%AAte%22+%23ForaMAMBolsonaro2018_2017-01-01_2017-12-31_20250728_165718.json\n",
      "ðŸ” Processing: search_response_%22La+b%C3%AAte%22_2017-01-01_2017-12-31_20250728_174921.json\n",
      "ðŸ” Processing: search_response_%22pelad%C3%A3o+do+MAM%22_2017-01-01_2017-12-31_20250728_181133.json\n",
      "ðŸ” Processing: search_response_%22Qual+vai+ser+a+desculpa+dessa+vez%3F%22+%28from%3Amblivre%29+lang%3Apt+until%3A2018-01-01+since%3A2017-09-01_2017-01-01_2017-12-31_20250731_163255.json\n",
      "ðŸ” Processing: search_response_%22Wagner+Schwartz%22+%22ped%C3%B3filo%22_2017-01-01_2017-12-31_20250728_173815.json\n",
      "ðŸ” Processing: search_response_%22Wagner+Schwartz%22+%28ped%C3%B3filo+OR+degenerado+OR+preso+OR+lixo+OR+doente%29_2017-01-01_2017-12-31_20250728_165425.json\n",
      "ðŸ” Processing: search_response_%22Wagner+Schwartz%22+OR+%22La+B%C3%AAte%22+29_2017-01-01_2017-12-31_20250728_120552.json\n",
      "ðŸ” Processing: search_response_%28%22liberdade+art%C3%ADstica%22+OR+%22censura%22+OR+%22arte+moderna2%29_2017-01-01_2017-12-31_20250728_155152.json\n",
      "ðŸ” Processing: search_response_%28%22pedofilia%22+OR+%22abusado2+OR+%22arte+degenerada%22%29+tz%22%29_2017-01-01_2017-12-31_20250728_124904.json\n",
      "ðŸ” Processing: search_response_%28from%3Amarcofeliciano%29+lang%3Apt+until%3A2017-10-01+since%3A2017-09-01_2017-01-01_2017-12-31_20250731_172658.json\n",
      "ðŸ” Processing: search_response_913531530923913221+lang%3Apt+until%3A2018-01-01+since%3A2017-09-01_2017-01-01_2017-12-31_20250731_170726.json\n",
      "ðŸ” Processing: search_response_913849428523601920+lang%3Apt+until%3A2018-01-01+since%3A2017-09-01_2017-01-01_2017-12-31_20250731_173116.json\n",
      "ðŸ” Processing: search_response_from%3Ajairbolsonaro+%28%22arte%22+OR+%22MAM%22+OR+%22crian%C3%A7a%22%29_2017-01-01_2017-12-31_20250728_164819.json\n",
      "ðŸ” Processing: search_response_from%3AMBLivre+%28%22La+B%C3%AAte%22+OR+%22Wagner+Schwartz%22+OR+%22MAM%22%29_2017-01-01_2017-12-31_20250728_155023.json\n",
      "ðŸ” Processing: search_response_from%3Ambl_brasil+%28%22La+B%C3%AAte%22+OR+%22Wagner+Schwartz%22+OR+%22MAM%22%29_2017-01-01_2017-12-31_20250728_154917.json\n",
      "ðŸ” Processing: search_response_La+B%C3%AAte+caso+completo_2017-01-01_2017-12-31_20250728_120328.json\n",
      "ðŸ” Processing: search_response_La+B%C3%AAte+fake+news+MBL_2017-01-01_2017-12-31_20250728_120435.json\n",
      "ðŸ” Processing: search_response_La+B%C3%AAte+pol%C3%AAmica+MAM+arte_2017-01-01_2017-12-31_20250728_120158.json\n",
      "ðŸ” Processing: search_response_MBL+MAM+arte_2017-01-01_2017-12-31_20250728_160840.json\n",
      "ðŸ” Processing: search_response_pedofilia+MAM+arte_2017-01-01_2017-12-31_20250728_180029.json\n",
      "ðŸ” Processing: search_response_performance+MAM+lang%3Apt+until%3A2018-01-01+since%3A2017-09-01_2017-01-01_2017-12-31_20250731_151339.json\n",
      "ðŸ” Processing: search_response_Wagner+Schwartz+MAM_2017-01-01_2017-12-31_20250728_175636.json\n",
      "ðŸ” Processing: search_response_Wagner+Schwartz_2017-01-01_2017-12-31_20250725_131206.json\n",
      "ðŸ” Processing: search_response_Wagner+Schwartz_2017-01-01_2017-12-31_20250729_113750.json\n",
      "ðŸ” Processing: search_response_Wagner+Schwartz_2017-01-01_2017-12-31_20250729_114825.json\n",
      "âœ… Done! 4525 tweets saved to ..\\data\\interim\\tweets_cleaned_wagner_schwartz.jsonl\n"
     ]
    }
   ],
   "source": [
    "#prepare tweets into organized json files\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dateutil import parser\n",
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "\n",
    "def clean_texts(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|pic\\.twitter\\.com\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove mentions and hashtags (optional for transformers)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # === Version 1: for Transformers ===\n",
    "    cleaned_text = text\n",
    "\n",
    "    # === Version 2: for Classic ML (BoW, TF-IDF) ===\n",
    "    text_bow = cleaned_text.lower()\n",
    "    text_bow = unidecode(text_bow)  # Remove accents\n",
    "    text_bow = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text_bow)  # Remove punctuation\n",
    "    text_bow = re.sub(r\"\\s+\", \" \", text_bow).strip()\n",
    "\n",
    "    return cleaned_text, text_bow\n",
    "\n",
    "\n",
    "def extract_tweets_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        tweets = []\n",
    "        seen_ids = set()\n",
    "        for item in data:\n",
    "            instructions = item.get(\"data\", {}).get(\"data\", {}).get(\"search_by_raw_query\", {}).get(\"search_timeline\", {}).get(\"timeline\", {}).get(\"instructions\", [])\n",
    "            if not instructions:\n",
    "                continue\n",
    "            entries = instructions[0].get(\"entries\", [])\n",
    "            for entry in entries:\n",
    "                if not entry.get(\"entryId\", \"\").startswith(\"tweet-\"):\n",
    "                    continue\n",
    "                tweet = entry.get(\"content\", {}).get(\"itemContent\", {}).get(\"tweet_results\", {}).get(\"result\", {})\n",
    "                legacy = tweet.get(\"legacy\", {})\n",
    "                tweet_id = tweet.get(\"rest_id\")\n",
    "                if not tweet_id or tweet_id in seen_ids:\n",
    "                    continue\n",
    "                full_text = legacy.get(\"full_text\", \"\").strip()\n",
    "                created_at = legacy.get(\"created_at\", \"\")\n",
    "                created_at_iso = \"\"\n",
    "                if created_at:\n",
    "                    try:\n",
    "                        created_at_iso = parser.parse(created_at).isoformat()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                likes = legacy.get(\"favorite_count\", \"\")\n",
    "                is_retweet = \"retweeted_status_result\" in tweet\n",
    "                is_quote = legacy.get(\"is_quote_status\", \"\")\n",
    "                in_reply_to = legacy.get(\"in_reply_to_screen_name\", \"\")\n",
    "                in_reply_to_status_id = legacy.get(\"in_reply_to_status_id_str\", \"\")\n",
    "                user = tweet.get(\"core\", {}).get(\"user_results\", {}).get(\"result\", {}).get(\"core\", {}).get(\"screen_name\")\n",
    "\n",
    "                user_mentions = legacy.get(\"entities\", {}).get(\"user_mentions\", [])\n",
    "                mentions = []\n",
    "                if user_mentions:\n",
    "                    for mention in user_mentions:\n",
    "                        mentions.append({\n",
    "                            \"id_str\": mention.get(\"id_str\", \"\"),\n",
    "                            \"name\": mention.get(\"name\", \"\"),\n",
    "                            \"username\": mention.get(\"screen_name\", \"\"),\n",
    "                        })\n",
    "\n",
    "                if full_text:\n",
    "                    cleaned, cleaned_bow = clean_texts(full_text)\n",
    "                    seen_ids.add(tweet_id)\n",
    "                    tweets.append({\n",
    "                        \"id\": tweet_id,\n",
    "                        \"user\": user,\n",
    "                        \"text\": full_text,\n",
    "                        \"cleaned_text\" : cleaned,\n",
    "                        \"cleaned_text_bow\" :cleaned_bow,\n",
    "                        \"created_at\": created_at,\n",
    "                        \"created_at_iso\" : created_at_iso,\n",
    "                        \"stance\" : \"\",\n",
    "                        \"is_quote\" : is_quote,\n",
    "                        \"is_retweet\": is_retweet,\n",
    "                        \"tweet_type\": \"retweet\" if is_retweet else (\"quote\" if is_quote else \"original\"),\n",
    "                        \"in_reply_to_user\": in_reply_to,\n",
    "                        \"in_reply_to_status_id\": in_reply_to_status_id,\n",
    "                        \"like_count\": likes,\n",
    "                        \"hashtags\" : [tag[\"text\"] for tag in legacy.get(\"entities\", {}).get(\"hashtags\", [])],\n",
    "                        \"mentions\" : mentions,\n",
    "                        \"retweet_count\": legacy.get(\"retweet_count\", 0),\n",
    "                        \"quote_count\": legacy.get(\"quote_count\", 0),\n",
    "                        \"reply_count\": legacy.get(\"reply_count\", 0)\n",
    "                    })\n",
    "        return tweets\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "all_tweets = []\n",
    "raw_folder = Path(\"../data/raw/WagnerSchwartz\")\n",
    "\n",
    "for file_path in raw_folder.glob(\"*.json\"):\n",
    "    print(f\"ðŸ” Processing: {file_path.name}\")\n",
    "    tweets = extract_tweets_from_file(file_path)\n",
    "    all_tweets.extend(tweets)\n",
    "\n",
    "# Write to .jsonl\n",
    "output_path = Path(\"../data/interim/tweets_cleaned_wagner_schwartz.jsonl\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as out:\n",
    "    for tweet in all_tweets:\n",
    "        json.dump(tweet, out, ensure_ascii=False)\n",
    "        out.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… Done! {len(all_tweets)} tweets saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a4b4a-d4b7-4fe2-9a3f-c4545f521e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8db2ae4-7fd0-43ad-bd22-46992f6ca693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4525 lines.\n",
      "Updated with stance: 2026\n",
      "No stance found for id: 2499\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_labelstudio_stances(ls_json_path):\n",
    "    \"\"\"\n",
    "    Build a dict: tweet_id (string) -> stance (string) from a Label Studio\n",
    "    export like the example you sent.\n",
    "    If multiple annotations exist, we pick the one with the latest updated_at.\n",
    "    \"\"\"\n",
    "    with open(ls_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    stance_by_id = {}\n",
    "\n",
    "    for item in data:\n",
    "        # tweet id is under item[\"data\"][\"id\"]\n",
    "        tweet_id = str(item.get(\"data\", {}).get(\"id\", \"\")).strip()\n",
    "        if not tweet_id:\n",
    "            continue\n",
    "\n",
    "        annots = item.get(\"annotations\", []) or []\n",
    "        if not annots:\n",
    "            continue\n",
    "\n",
    "        # choose the latest annotation by updated_at (fallback to created_at)\n",
    "        def _parse_dt(annot):\n",
    "            ts = annot.get(\"updated_at\") or annot.get(\"created_at\")\n",
    "            try:\n",
    "                return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\")) if ts else datetime.min\n",
    "            except Exception:\n",
    "                return datetime.min\n",
    "\n",
    "        annots_sorted = sorted(annots, key=_parse_dt, reverse=True)\n",
    "        chosen = annots_sorted[0]\n",
    "\n",
    "        # extract stance from the first result->value->choices[0]\n",
    "        stance = None\n",
    "        for res in chosen.get(\"result\", []) or []:\n",
    "            val = res.get(\"value\", {})\n",
    "            choices = val.get(\"choices\", [])\n",
    "            if choices:\n",
    "                stance = choices[0]\n",
    "                break\n",
    "\n",
    "        if stance is not None:\n",
    "            stance_by_id[tweet_id] = stance\n",
    "\n",
    "    return stance_by_id\n",
    "\n",
    "\n",
    "def merge_stance_into_jsonl(ls_json_path, jsonl_in_path, jsonl_out_path):\n",
    "    # build stance dict from Label Studio JSON\n",
    "    id2stance = parse_labelstudio_stances(ls_json_path)\n",
    "\n",
    "    total = 0\n",
    "    updated = 0\n",
    "    missing = 0\n",
    "\n",
    "    with open(jsonl_in_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(jsonl_out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        for line in fin:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            total += 1\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            tid = str(obj.get(\"id\", \"\")).strip()\n",
    "            if tid and tid in id2stance:\n",
    "                obj[\"stance\"] = id2stance[tid]\n",
    "                updated += 1\n",
    "            else:\n",
    "                # keep line as-is if no stance for this id\n",
    "                missing += 1\n",
    "\n",
    "            fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Processed {total} lines.\")\n",
    "    print(f\"Updated with stance: {updated}\")\n",
    "    print(f\"No stance found for id: {missing}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "merge_stance_into_jsonl(\n",
    "    \"../data/processed/20250909_classified_tweets.json\",\n",
    "    \"../data/interim/tweets_cleaned_wagner_schwartz.jsonl\",\n",
    "    \"../data/interim/tweets_cleaned_wagner_schwartz_with_stance.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3904e9-4088-4907-8436-f0f4ba568b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
