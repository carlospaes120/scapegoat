{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fce216c-e11c-49f1-a9ce-beb1902c3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def criar_arquivo_gephi_temporal(caminho_arquivo_jsonl, arquivo_saida_gexf):\n",
    "    \"\"\"\n",
    "    Cria um arquivo .gexf temporal para o Gephi.\n",
    "    Nós = usuários; Arestas = interações (menções/respostas) com stance e tempo.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    # Grafo dinâmico + formato de tempo em datetime ISO\n",
    "    G.graph[\"mode\"] = \"dynamic\"\n",
    "    G.graph[\"timeformat\"] = \"datetime\"  # Gephi lê 'start'/'end' como strings ISO\n",
    "\n",
    "    def to_iso_z(ts: str) -> str:\n",
    "        # Aceita \"....Z\" ou com offset; zera microssegundos e força 'Z' (UTC)\n",
    "        dt = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "        dt = dt.replace(microsecond=0)\n",
    "        # Gephi lida bem com 'Z'\n",
    "        return dt.isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "    with open(caminho_arquivo_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                tweet = json.loads(line)\n",
    "                autor = tweet.get(\"user\")\n",
    "                stance = tweet.get(\"stance\")\n",
    "                timestamp_str = tweet.get(\"created_at_iso\")\n",
    "                if not autor or not stance or not timestamp_str:\n",
    "                    continue\n",
    "\n",
    "                valid_timestamp = to_iso_z(timestamp_str)\n",
    "\n",
    "                # garante existência de nó do autor\n",
    "                G.add_node(autor)\n",
    "\n",
    "                # coletar alvos da interação (menções + reply-to)\n",
    "                mencoes = [m.get(\"username\") for m in tweet.get(\"mentions\", []) if m.get(\"username\")]\n",
    "                usuario_respondido = tweet.get(\"in_reply_to_user\")\n",
    "                if usuario_respondido:\n",
    "                    mencoes.append(usuario_respondido)\n",
    "\n",
    "                # criar arestas com tempo e stance\n",
    "                for mencionado in set(mencoes):\n",
    "                    if not mencionado:\n",
    "                        continue\n",
    "                    G.add_node(mencionado)\n",
    "                    # start=end => evento instantâneo naquele timestamp\n",
    "                    G.add_edge(\n",
    "                        autor,\n",
    "                        mencionado,\n",
    "                        stance=stance,\n",
    "                        start=valid_timestamp,\n",
    "                        end=valid_timestamp,\n",
    "                    )\n",
    "            except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "                print(f\"Erro ao processar linha: {line.strip()}. Erro: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Grafo temporal criado com {G.number_of_nodes()} nós e {G.number_of_edges()} arestas.\")\n",
    "\n",
    "    try:\n",
    "        # Deixe no default (1.2draft) ou especifique '1.2draft'\n",
    "        nx.write_gexf(G, arquivo_saida_gexf)  # ou version='1.2draft'\n",
    "        print(f\"Arquivo '{arquivo_saida_gexf}' criado com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao salvar o arquivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d33a2b-3be8-4396-bd15-df6ae449ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo temporal criado com 1098 nós e 848 arestas.\n",
      "Arquivo 'eduardo_bueno_temporal.gexf' criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# --- Como usar a função ---\n",
    "# Certifique-se que este arquivo existe e contém os dados no formato esperado\n",
    "arquivo_de_tweets_classificados = 'tweets_classified_wagner_schwartz.jsonl' \n",
    "nome_do_arquivo_gephi_temporal = 'wagner_schwartz_temporal.gexf'\n",
    "\n",
    "criar_arquivo_gephi_temporal(arquivo_de_tweets_classificados, nome_do_arquivo_gephi_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e53063e-8744-4547-8782-930f468adbd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m row[\u001b[33m\"\u001b[39m\u001b[33mmentions\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m v != u:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m             \u001b[43madd_user_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m             add_edge(u, v, W_MENTION, \u001b[33m\"\u001b[39m\u001b[33mmention\u001b[39m\u001b[33m\"\u001b[39m, time=t)\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Quotes (if you have a quoted user field; many datasets don’t—kept here for completeness)\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Example placeholder: row.get(\"quoted_user\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36madd_user_if_needed\u001b[39m\u001b[34m(u, stance)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_user_if_needed\u001b[39m(u, stance=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m G.has_node(u):\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[43mG\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stance:\n\u001b[32m     84\u001b[39m             G.nodes[u][\u001b[33m\"\u001b[39m\u001b[33mstance\u001b[39m\u001b[33m\"\u001b[39m] = stance\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leoci\\OneDrive\\Área de Trabalho\\scapegoat\\.venv\\Lib\\site-packages\\networkx\\classes\\digraph.py:478\u001b[39m, in \u001b[36mDiGraph.add_node\u001b[39m\u001b[34m(self, node_for_adding, **attr)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_for_adding, **attr):\n\u001b[32m    440\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Add a single node `node_for_adding` and update node attributes.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    476\u001b[39m \u001b[33;03m    doesn't change on mutables.\u001b[39;00m\n\u001b[32m    477\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnode_for_adding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_succ\u001b[49m:\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m node_for_adding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    480\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNone cannot be a node\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "\n",
    "# network_builder.py\n",
    "# Build a user-user interaction network (reply / retweet / mention) from your JSONL\n",
    "# and export to GEXF for Gephi (with stance + metrics on nodes and edge weights).\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "INPUT_JSONL = 'tweets_classified_wagner_schwartz.jsonl' # adjust path if needed\n",
    "OUTPUT_GEXF = 'wagner_schwartz_temporal.gexf'\n",
    "OUTPUT_CSV_NODES = \"network_nodes.csv\"\n",
    "OUTPUT_CSV_EDGES = \"network_edges.csv\"\n",
    "\n",
    "# Choose which interactions to include as edges\n",
    "INCLUDE_REPLIES = True\n",
    "INCLUDE_RETWEETS = True\n",
    "INCLUDE_MENTIONS = True\n",
    "INCLUDE_QUOTES = True   # will only be used if we have a quoted user field\n",
    "\n",
    "# Edge weighting strategy: how much to weight each interaction type\n",
    "W_REPLY = 1.0\n",
    "W_RETWEET = 1.0\n",
    "W_MENTION = 0.5\n",
    "W_QUOTE = 1.0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load\n",
    "# -----------------------------\n",
    "def load_jsonl(path):\n",
    "    recs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                recs.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "\n",
    "df = load_jsonl(INPUT_JSONL)\n",
    "\n",
    "# Normalize/ensure expected columns exist\n",
    "for col in [\n",
    "    \"user\", \"stance\", \"is_retweet\", \"is_quote\", \"in_reply_to_user\",\n",
    "    \"mentions\", \"created_at_iso\", \"like_count\", \"retweet_count\",\n",
    "    \"reply_count\", \"quote_count\", \"id\"\n",
    "]:\n",
    "    if col not in df.columns:\n",
    "        if col == \"mentions\":\n",
    "            df[col] = [[] for _ in range(len(df))]\n",
    "        else:\n",
    "            df[col] = None\n",
    "\n",
    "# Parse datetime\n",
    "if \"created_at_iso\" in df.columns:\n",
    "    df[\"created_at_iso\"] = pd.to_datetime(df[\"created_at_iso\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build directed graph\n",
    "# -----------------------------\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Mark dynamic/time metadata so Gephi recognizes temporal attributes (optional)\n",
    "G.graph[\"mode\"] = \"dynamic\"\n",
    "G.graph[\"timeformat\"] = \"datetime\"\n",
    "\n",
    "# Aggregate edge weights before adding to graph (reduces duplicates)\n",
    "edge_accumulator = defaultdict(lambda: {\"weight\": 0.0, \"types\": defaultdict(int)})\n",
    "\n",
    "def add_user_if_needed(u, stance=None):\n",
    "    if not G.has_node(u):\n",
    "        G.add_node(u)\n",
    "        if stance:\n",
    "            G.nodes[u][\"stance\"] = stance\n",
    "\n",
    "def add_edge(u, v, w, etype, time=None):\n",
    "    key = (u, v)\n",
    "    edge_accumulator[key][\"weight\"] += w\n",
    "    edge_accumulator[key][\"types\"][etype] += 1\n",
    "    # Optional: keep earliest interaction time for temporal start\n",
    "    if time is not None:\n",
    "        if \"start\" not in edge_accumulator[key]:\n",
    "            edge_accumulator[key][\"start\"] = time\n",
    "        else:\n",
    "            # keep the earliest\n",
    "            edge_accumulator[key][\"start\"] = min(edge_accumulator[key][\"start\"], time)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    u = row.get(\"user\")\n",
    "    if not u:\n",
    "        continue\n",
    "\n",
    "    stance = row.get(\"stance\") or \"neutro\"\n",
    "    add_user_if_needed(u, stance=stance)\n",
    "\n",
    "    t = row.get(\"created_at_iso\")\n",
    "\n",
    "    # Replies (u -> replied_user)\n",
    "    if INCLUDE_REPLIES and row.get(\"in_reply_to_user\"):\n",
    "        v = row[\"in_reply_to_user\"]\n",
    "        if v:\n",
    "            add_user_if_needed(v)  # stance unknown for targets we haven't seen tweeting\n",
    "            add_edge(u, v, W_REPLY, \"reply\", time=t)\n",
    "\n",
    "    # Retweets\n",
    "    # NOTE: In some datasets, the original author isn't provided. If your JSONL\n",
    "    # encodes the original author elsewhere (e.g., \"retweeted_user\"), swap in here.\n",
    "    if INCLUDE_RETWEETS and row.get(\"is_retweet\") and row.get(\"in_reply_to_user\"):\n",
    "        v = row[\"in_reply_to_user\"]\n",
    "        if v:\n",
    "            add_user_if_needed(v)\n",
    "            add_edge(u, v, W_RETWEET, \"retweet\", time=t)\n",
    "\n",
    "    # Mentions (u -> each mentioned @user)\n",
    "    if INCLUDE_MENTIONS and isinstance(row.get(\"mentions\"), list):\n",
    "        for v in row[\"mentions\"]:\n",
    "            if v and v != u:\n",
    "                add_user_if_needed(v)\n",
    "                add_edge(u, v, W_MENTION, \"mention\", time=t)\n",
    "\n",
    "    # Quotes (if you have a quoted user field; many datasets don’t—kept here for completeness)\n",
    "    # Example placeholder: row.get(\"quoted_user\")\n",
    "    quoted_user = row.get(\"quoted_user\")\n",
    "    if INCLUDE_QUOTES and row.get(\"is_quote\") and quoted_user:\n",
    "        v = quoted_user\n",
    "        add_user_if_needed(v)\n",
    "        add_edge(u, v, W_QUOTE, \"quote\", time=t)\n",
    "\n",
    "# Add nodes’ numeric activity/engagement aggregates\n",
    "user_group = df.groupby(\"user\", dropna=True)\n",
    "for u, g in user_group:\n",
    "    if not G.has_node(u):\n",
    "        continue\n",
    "    G.nodes[u][\"tweets\"] = int(len(g))\n",
    "    G.nodes[u][\"likes_sum\"] = int(g[\"like_count\"].fillna(0).sum())\n",
    "    G.nodes[u][\"retweets_sum\"] = int(g[\"retweet_count\"].fillna(0).sum())\n",
    "    G.nodes[u][\"replies_sum\"] = int(g[\"reply_count\"].fillna(0).sum())\n",
    "    G.nodes[u][\"quotes_sum\"] = int(g[\"quote_count\"].fillna(0).sum())\n",
    "    # (Re)assign stance as the mode for users with mixed stances\n",
    "    mode_stance = g[\"stance\"].dropna().mode()\n",
    "    if len(mode_stance):\n",
    "        G.nodes[u][\"stance\"] = mode_stance.iloc[0]\n",
    "\n",
    "# Materialize edges in the graph\n",
    "for (u, v), info in edge_accumulator.items():\n",
    "    G.add_edge(u, v,\n",
    "               weight=float(info[\"weight\"]),\n",
    "               reply_count=int(info[\"types\"].get(\"reply\", 0)),\n",
    "               retweet_count=int(info[\"types\"].get(\"retweet\", 0)),\n",
    "               mention_count=int(info[\"types\"].get(\"mention\", 0)),\n",
    "               quote_count=int(info[\"types\"].get(\"quote\", 0)),\n",
    "               # temporal start for edge (first observed interaction)\n",
    "               start=info.get(\"start\").isoformat() if isinstance(info.get(\"start\"), pd.Timestamp) else None\n",
    "               )\n",
    "\n",
    "print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Centralities (on the directed graph)\n",
    "# -----------------------------\n",
    "# Use weight for degree-like metrics; for PR, weight is a 'weight' edge attribute\n",
    "in_deg = dict(G.in_degree(weight=\"weight\"))\n",
    "out_deg = dict(G.out_degree(weight=\"weight\"))\n",
    "nx.set_node_attributes(G, in_deg, \"in_strength\")\n",
    "nx.set_node_attributes(G, out_deg, \"out_strength\")\n",
    "\n",
    "try:\n",
    "    pr = nx.pagerank(G, weight=\"weight\", alpha=0.85, max_iter=100)\n",
    "    nx.set_node_attributes(G, pr, \"pagerank\")\n",
    "except nx.PowerIterationFailedConvergence:\n",
    "    print(\"PageRank didn't converge; skipping.\")\n",
    "\n",
    "btw = nx.betweenness_centrality(G, weight=\"weight\", normalized=True, k=None)\n",
    "nx.set_node_attributes(G, btw, \"betweenness\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Export: GEXF + CSVs for quick inspection\n",
    "# -----------------------------\n",
    "# Gephi sometimes complains if None appears in attributes; clean them\n",
    "for n, data in G.nodes(data=True):\n",
    "    for k, v in list(data.items()):\n",
    "        if v is None:\n",
    "            G.nodes[n][k] = \"\"\n",
    "\n",
    "for u, v, data in G.edges(data=True):\n",
    "    for k, v2 in list(data.items()):\n",
    "        if v2 is None:\n",
    "            G.edges[u, v][k] = \"\"\n",
    "\n",
    "nx.write_gexf(G, OUTPUT_GEXF)\n",
    "print(f\"Saved GEXF → {OUTPUT_GEXF}\")\n",
    "\n",
    "# Also dump flat CSVs\n",
    "nodes_rows = []\n",
    "for n, d in G.nodes(data=True):\n",
    "    row = {\"user\": n}\n",
    "    row.update(d)\n",
    "    nodes_rows.append(row)\n",
    "\n",
    "edges_rows = []\n",
    "for u, v, d in G.edges(data=True):\n",
    "    row = {\"source\": u, \"target\": v}\n",
    "    row.update(d)\n",
    "    edges_rows.append(row)\n",
    "\n",
    "pd.DataFrame(nodes_rows).to_csv(OUTPUT_CSV_NODES, index=False)\n",
    "pd.DataFrame(edges_rows).to_csv(OUTPUT_CSV_EDGES, index=False)\n",
    "print(f\"Saved nodes CSV → {OUTPUT_CSV_NODES}\")\n",
    "print(f\"Saved edges CSV → {OUTPUT_CSV_EDGES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee1d60-b166-40a2-9e0c-889cd88f2642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
